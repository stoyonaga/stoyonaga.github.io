---
title: "PostgreSQL: Generated Tuning Hints with Large-Language Models by Exploiting NLP"
layout: page
date: 2023-12-14 14:00
tag: Machine Learning
projects: true
hidden: true
description: "Using Machine Learning models to identify and/or generate PostgreSQL-related tuning hints on systems of varying hardware/software architectures."
category: project
author: shogz
externalLink: false
---
<h1>Generated Tuning Hints with Large-Language Models by Exploiting NLP</h1>

<div class="side-by-side">
    <div class="toleft">
        <p><strong><em>Introduction:</em></strong> Two machine learning models were developed in an effort to reduce the cognitive load on Database Administrators when fine-tuning a PostgreSQL database system. The first model answers simple questions by utilizing embeddings on a custom-tailored knowledge-base consisting of user manuals, YouTube transcripts, and Reddit/Quora posts. The second model uses a Generative Pre-trained Transformer 2 (GPT-2) model with 124M parameters to answer more complex queries that require understanding of semantic relationships and dependencies between parameter non-independence. The training data to fine-tune the model was generated by ChatGPT-4 and the author by leveraging the first models input-output pairs.</p>
    </div>
    <div class="toright">
        <img class="image" src="https://images.pexels.com/photos/5203849/pexels-photo-5203849.jpeg?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1" alt="Alt Text">
    </div>
    <h2>Technical Details</h2>
    <ol>
        <li><strong>Model 1: </strong><em>HayStacks</em> was used to configure the question-answer pipeline that leveraged sentence-similarity tasks and data preparation/preprocessing from a user-defined knowledge-base.</li>
        <li><strong>Model 2: </strong><em>SFTTrainer</em> was used to fine-tune the GPT2 model on relevant question-answer pairs.</li>
        <li><strong>Web Scraping: </strong><em>PlayWright, BeautifulSoup4</em> were used to webscrape tuning-hints from Reddit/Quora and save them into the user-defined knowledge-base as text files. Additionally, <em>youtube-transcript-api</em> was used to obtain text files that contained transcripts from conferences that discussed tuning-related hints.</li>
        <li><strong>External Tools: </strong><em>ChatGPT4</em> was used to help generate training data to finetune the LLM.</li>
    </ol>
    <h2>Source Code & Paper</h2>
    <ol>
        <li>The source code was developed on Google Colab. You can access the notebook and dataset <a href = "https://github.com/stoyonaga/EECS6415_LLMs">here</a>.</li>
        <li>To access additional files (i.e., written report), please feel free to reach out to me!! :)</li>
    </ol>
</div>